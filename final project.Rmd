---
title: "Stat 486 Final Report"
author: "Xiaohan Liu"
date: "Dec 14th, 2017"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r c0, echo = F, warning=F, message=F, error=F}

library(data.table)
library(MASS)
library(car)
library(ggplot2)
library(ggrepel)
library(ggpubr)
library(ggExtra)
library(stargazer)
library(nlme)
library(corrplot)
library(Matrix)
library(olsrr)
library(eply)
library(grid)
setwd("C:/Users/Han/Desktop/Box Sync/Stat 501/Final Project")

dt_1 = fread('final project dataset 1.txt')
dt_2 = fread('final project dataset 2.txt')

dt = merge(dt_1,dt_2,by = 'State')

tr <- function (m)
{
   total_sum <- 0
   if(is.matrix(m))
   {
      row_count <- nrow(m)
      col_count <- ncol(m)
      if(row_count == col_count)
      {
         total_sum <-sum(diag(m))
         total_sum
      }
      else
      {
         message ('Matrix is not square')
      }
   }
   else
   {
      message( 'Object is not a matrix')
      
   }
}
Near.Neighbor.Approach = function(x){
   x = as.data.frame(x)
   rns = x[,1]
   x = x[,-1]
   rownames(x) = rns
   x = as.matrix(x)
   
   n = nrow(x) ; k = ncol(x)
   y = x[,1]
   z = matrix(1,n,1)
   x[,1] = z
   
   prx = diag(n) - x %*% ginv(t(x)%*%x) %*% t(x)
   E = prx %*% y
   V = matrix(0,n,n)
   for (i in 1:(n-1)){
      for (j in (i+1):n){
         d = t(x[i,]-x[j,]) %*% (x[i,]-x[j,])
         if (d > 0) {
            V[i,j] = -2/d
            V[j,i] = -2/d
         }
      }
   }
   
   for (i in 1:n){
      V[i,i] = - sum(V[i,])
   }
   
   B = prx %*% V %*% prx
   
   NUM = t(E) %*% V %*% E
   DEN = t(E) %*% E/(n-k)
   STAT = NUM/DEN
   MEAN = tr(B)
   VAR = 2*((n-k)*tr(B %*% B)-(tr(B))^2)/(n-1)
   STAT = (STAT -MEAN)/sqrt(VAR)
   STAT = as.numeric(STAT)
   return(STAT)
}

dt1 = dt[,c('State','M','MA','D','PL','S','B','HT','UR','CR','HS','INC','VT')]
dt1[,2:13] = dt1[,2:13][,lapply(.SD,as.numeric)]
# standardize all X
dt1[,3:13] = dt1[,3:13][,lapply(.SD,scale)]

t.1 = Near.Neighbor.Approach(dt1)
n = 50; k = 11
p_value = 2* pt(t.1,n-k+2)

dt2 = cbind(dt[,15],dt[,-15])
dt2 = dt2[,c(2,1,3:27)]
dt2[,2:27] = dt2[,2:27][,lapply(.SD,as.numeric)]
# standardize all X
dt2[,3:27] = dt2[,3:27][,lapply(.SD,scale)]

t.2 = Near.Neighbor.Approach(dt2)
n = 50; k = 25
p_value = 2* pt(t.2,n-k+2)


dt2.a = dt2
dt2.a[,2] = 1/dt2.a[,2]

t.2.a = Near.Neighbor.Approach(dt2.a)
n = 50; k = 25
p_value = 2* pt(t.2.a,n-k+2)
colnames(dt2.a)[2] = 'inv_MA'

dt2.b = dt2.a
dt2.b[,6:8] = dt2.b[,6:8][,lapply(.SD,rank)]
dt2.b[,6:8] = dt2.b[,6:8]/50
t.2.b = Near.Neighbor.Approach(dt2.b)
p_value = 2* (1-pt(t.2.b,n-k+2))
```

1. Introduction  

The study was based on a two compiled datasets from Statistical Abstract of the United States, 1981, U.S. Bureau of the Census, Washington, D.C. It provided some basic sociological information in 26 metrics for each of the 50 states, and they are listed in Table 1. Two major concerns were raised by our respected clients: 1. Find an appropriate linear regression model with M as the dependent variable along with MA, D, S, B, HT, UR, CR, HS, INC, PL and VT to choose from as independent variables; 2. Find an appropriate linear regression model with MA as the dependent variable along with all other variables to choose from as independent variables.  

2. Methodology

All of our statistical analysis was conducted on the platform R 3.4.3 and Jupyterlab 0.27.0 (Python 3.5.2). The procedures were similar between the two topics, the primary difference lied in the transformation.   

2.1. Exploratory Data Analysis:  

EDA techniques were used to explore the data quality at the first glimpse. Summary statistics of all variables could be found in Table 2, with their graphical displays shown in Graph 1 and Graph 2. 
```{r T1, echo = F}
stargazer(dt, type = 'text', 
          column.sep.width = '15pt',
          align = T, digits = 2)
```
![Graph 1. Box plots of all studentized variables and Histograms of all variables](g1.png)  

![Graph 2. Scatter plot on M and 1/MA from each state](g2.tiff)  

2.2 Test of Model adequacy:  

Starting from this step, we applied standardization on each candidate independent variable respectively to ease further computations and it should change our regression results. In order to know if some transformation on the independent variables and/or dependent variable is necessary, we were using the Near-neighbor approach to test our current model adequacy. The test statistic T for it is:
$$
T =\frac{\Omega - E(\Omega)}{Var(\Omega)} \sim t(n-p-1), \ and \ \  \Omega = \frac{e'Ve}{s^2}
$$
where,
$$
V_{il} = V_{li} =  - 2\omega_{il}; \ and \ \ V_{ii}  = \sum_lV_{il}
$$
$$
\omega_{il} = \omega_{li}= \frac{1}{(||x_i - x_l||)^2}, \ and\  \omega_{ii} = 0\ \  \forall i,l \in\ \{1, \cdots, p+1 \}
$$
We had also found that:
$$
E(\Omega) = tr(B); \ \ and \ Var(\Omega) = \frac{2[(n-p-1)tr(B'B)-(tr(B))^2]}{n-1}
$$
where,
$$
B = M'VM; \ and \ \ M = I - H_X
$$
And, we would reject the model adequacy if $|T|>t_{\alpha/2,n-p-1},\ \alpha = 0.05$.   

2.3. Transformation

If the current model fails the above adequacy test, we would do some transformation, such as the derivatives of BoxCox power transformation, on its response first and then on the independent variables if necessary until the transformed model passed the near-neighbor approach test for adequacy.

  
2.4. Step-wise variable selection  

After we obtained an adequate model, we would perform dimension reduction by a step-wise both-direction method with p-values and AIC as our primary criteria along with adj-R^2^ and C(p) as secondary criteria.   

2.5. Multicollinearity diagnostics  

Upon the model with selected variables, we would utilize VIF (Variance Inflation Factor) and Condition Index side by side to detect the variable(s) influenced by multicollinearity. Our criteria were: From variables whose VIF are large than 10 or Condition Index are larger than 40,  we would delete the variable with the largest VIF and Condition Index until none exists. Here, we choose to simply delete variable is given our number of observation is relatively small compared to the number of independent variables.   

2.6. Linear regression, residual diagnostics and influence measures    

Based on the eventually screened out independent variable, we would do a linear regression with all interaction terms automatically into consideration. Then we would test the linearity by the residual vs. fitted values plots, the normality by qqplot along with Shapiro-Wilk test and Kolmogorov-Smirnov test. Besides, we would carry out the measures of influence via Cook's D plot, studentized residual plot, deleted studentized residual vs. fitted values plot and studentized residuals vs leverage plot to detect the influential points and outliers.

3. Results and Discussion  

3.1. Find an appropriate linear regression model with M as the dependent variable along with MA, D, S, B, HT, UR, CR, HS, INC, PL and VT to choose from as independent variables.  

![Graph 3. Partial scatter plots in 1st problem](g3.tiff)   

In the initial EDA, we checked the partial scatter plots of each candidate independent variable versus the responses like in Graph 3. Aside from one influential point in MA screwing up the range of the entire plot, all other plots look decent enough, hinting no need for transformation. And it was checked by another partial plot between influential point temporarily removed MA and the response M, and the same could be expected for variable D. We then carried out the model adequacy test by the Near-neighbor approach, and got a well above any common significance level p-value of 0.50 (code and outputs could be find in Code 1). This was in agreement with our previous guess that no transformation was required on the as-is linear model.  

Next, we proceeded with the variable selection from all candidates following a step-wise both-direction fashion evaluated by various different criteria. The results are listed below in Table 2. :
```{r t2, echo=F, warning=F}
model1 = lm(M ~., data = dt1[,2:13])
k1 = ols_stepwise(model1)
k1
```

And we also plotted the above variable selection result in Graph 4.   

![Graph 4. Stewise regression plots in 1st problem](g4.tiff)    

As indicated in methodology, we followed p-values and AIC as our primary criteria along with adj-R^2^ and C(p) as secondary criteria. Meanwhile, we were fully aware of that step-wise regression could miss the "best" model. Thus,after consulted with the best subset model selected from all 2^11^ possible models (shown in Table 3. and Graph 5) and the real life meaning of the variables, the final model we landed on was:
$$
M \sim D + PL + UR + HS + INC + VT + \epsilon
$$

```{r t3, echo=F, warning=F}
dt1 = dt[,c('State','M','MA','D','PL','S','B','HT','UR','CR','HS','INC','VT')]
dt1[,2:13] = dt1[,2:13][,lapply(.SD,as.numeric)]
# standardize all X
dt1[,3:13] = dt1[,3:13][,lapply(.SD,scale)]
model1 = lm(M ~., data = dt1[,2:13])
k1.a = ols_best_subset(model1)
k1.a
```

![Graph 5. Correlation matrix of the chose model for 1st problem](g5.tiff)  

In addition, we tested the chosen model's multicollinearity in Table 4. All variables' VIFs were well below 10 and condition indices were well below 30. Thus, we could say this chosen model didn't possess a prominent multicollinearity issue. 
```{r t4, echo=F,warning=F}
dt1.b = dt1[,c(2,4,5,9,11:13)]
# multicollinearity
model1.b = lm(M ~., data = dt1.b)
ols_coll_diag(model1.b)
```

Then, the chosen model's linear regression result was displayed in Table 5, including the model summary, ANOVA table and the parameter estimates including the 95% C.I.. It should be noted although we included all the interaction terms but none was significant, meaning all the independent variables tended to influence the response independently. The adjusted R^2^ value was 0.843, demonstrating a high level goodness-of-fit. From the residual diagnostic (Graph 6.), we could observe no clear pattern, indicating the validity of the linearity. The normality assumption was also supported by the qqplot and histogram, and further corroborated by S-W test (p-value:0.919) and K-S test (p-value:0.914). Meanwhile, form the influence measures (Graph 7.), we found observation 33 (NV) stray furthest from the herd in both Cook's D and Leverage metric. Under closer examination, it was mainly due to its absurdly high divorce rate (studentized value equals 4.87), which could be justified probably only by the very existence of Las Vegas and its one-of-a-kind social ecological model. Furthermore, DFBETAs (Graph 8.) measured the difference in each coefficient estimate with and without the influential point. From which, we could get senses on how much one observation has effected the estimate of a regression coefficient. Clearly, apart from the NV, observation 39 (RI) also looked have huge influences on variable PL, UR, HS, INC and VT. On the other hand, we should expect 2.5 outliers on average out of our 50 observations. In practice, 4 were found (Graph 7.), and our dear clients should treat them with extra care. In particular, the "worst" one of them, possessing both the largest leverage and studentized residual was observation 39 (RI), a.k.a the tiniest state of all, was not extremely surprising. Hence, if it was necessary to remove this outlier, it might result in one of the slightest possible impacts to the validity of this model nation-wide.   
```{r t5,echo=F}
ols_regress(M~D+PL+UR+HS+INC+VT, data = dt1.b,iterm = T)
```

![Graph 6. Residual diagnostic of the chose model for 1st problem](g6.tiff)   

![Graph 7. Influential point and outlier diagnostic of the chose model for 1st problem](g7.png)   

![Graph 8. DFBETAs panel of the chose model for 1st problem](g8.tiff)    

When doing the inference on the effect sizes predicted by our built model (Table 5), we discovered that the Per mil of population below poverty level (PL) and Per mil high school grads (HS) serve as the most and the least impactful independent variable respectively to Murder rate per 100,000 population (M), whilst other effect sizes were close the HS'. On the aspect of the sign of the effects, it made perfect sense that a higher Divorce rate per 10,000 (D), a higher PL, a higher Per mil of population living in urban areas (UR), a lower HS, and a lower % voting for presidential candidates among voting age population (VT) would stimulate a higher murder rate (M). However, the reasoning of a higher Per capita income expressed in 1972 dollars (INC), which also demonstrated a strong negative correlation to PL, would increase the M didn't come around naturally. Maybe because INC was like a mean instead of a median, which traditionally considered as a better representation for the mass' income. Either way, this issue should be brought to our respected client's full attention.    
   
3.2. Find an appropriate linear regression model with MA as the dependent variable along with all other variables to choose from as independent variables.  

In the initial EDA, we checked the partial scatter plots of each candidate independent variable versus the 1/response like in Graph 9. Aside from several influential points for a few independent variables like D and PR, all other plots look decent enough, hinting the transformation of inverse the as-is response MA. We then carried out the model adequacy test by the Near-neighbor approach, and got a well below any common significance level p-value of 7.3e-4 (code and outputs could be find in Code 1). It proved our initial guess that some transformation was indeed required on the as-is linear model.    

![Graph 9. Partial scatter plots in 2nd problem](g9.png)   

To address this issue, we first tried to do a Boxcox power transformation on our response, the plot was shown in Graph 10. -1 was inside the 95% C.I. of the max log-likelihood function, hence we decided to do an inverse transformation on our response MA. The corresponding Near-neighbor approach model adequacy test rendered a p-value of 0.184, indicating merely inverting the MA would make the model adequate for the ensued procedure. This was also the very reason we started off by showing partial plots between the inverse of MA versus all other independent variables. We then decided to do some additional transformation of ranking on Number of blacks (1000’s) (BL), Number of Spanish speaking (1000’s) (SP), and Number of Native Americans (1000’s) (AI). And this procedure indeed improved the Near-neighbor approach model adequacy test with an updated p-value of 0.494. 

![Graph 10. Boxcox power transformation on the response plot in 2nd problem](g10.tiff)   

Next, we proceeded with the variable selection from all candidates following a step-wise both-direction fashion evaluated by various different criteria. The results are listed below in Table 6. However the outputs was only D, B, INC and DI four variables with a moderate adjusted R^2^ values and pretty abysmal C(p) values, we continued to seek a more fitting model. On the other hand, it should be noted that we have 25 independent variables so there were $2^{25} > 3.3E7$ possible models, which was too much for my gear to handle. After running several variables selections with different criteria and taking their sociological meanings into consideration, we decided to run a overall best subset model screening from 15 independent variables. However, as we could see, the best model remained unchanged, which was:
$$
\frac{1}{MA} = D + B + INC + DI +\epsilon
$$
And the correlation table was displayed in Graph 11.   

```{r t6, echo=F, warning=F}
model2.b = lm(inv_MA ~., data = dt2.b[,2:27])
k2.b = ols_stepwise(model2.b)
k2.b
```

![Graph 11. Correlation matrix of the chose model for 2nd problem](g11.tiff)   

In addition, we tested the chosen model's multicollinearity in Table 7. All variables' VIFs were well below 10 and condition indices were well below 30. This was not surprised at all given we only have four independent variables left. Thus, we could say for sure this chosen model didn't possess a prominent multicollinearity issue.   

```{r t7, echo=F,warning=F}
dt2.d = dt2.a[,c(2,12,15,16,26)]
# multicollinearity
model2.d = lm(inv_MA~., data = dt2.d)
ols_coll_diag(model2.d)
```

Then, the chosen model's linear regression result was displayed in Table 8, including the model summary, ANOVA table and the parameter estimates including the 95% C.I.. It should be noted although we included all the interaction terms but none was significant, meaning all the independent variables tended to influence the response independently. The adjusted R^2^ value was 0.675, demonstrating a fairly high level goodness-of-fit. From the residual diagnostic (Graph 12.), we could observe no clear pattern, indicating the validity of the linearity. The normality assumption was also supported by the qqplot and histogram, and further corroborated by S-W test (p-value:0.319) and K-S test (p-value:0.692). Meanwhile, form the influence measures (Graph 13.), we found observation 33 (NV) still stray furthest from the herd in both Cook's D and Leverage metric for the same reason stated in 3.1. Furthermore, DFBETAs (Graph 14.) measured the difference in each coefficient estimate with and without the influential point. From which, we could get senses on how much one observation has effected the estimate of a regression coefficient. Clearly, apart from the NV, observation 27 (NC) also showed huge influences on variable B, DI and INC. Observation 37 (OR) also showed huge influences on variable DI. Observation 40 (SC) also showed huge influences on variable INC. Observation 44 (UT) also showed huge influences on variable B. On the other hand, we should expect 2.5 outliers on average out of our 50 observations. In practice, 4 were found (Graph 13.), and our dear clients should treat them with extra care. In particular, the "worst" one of them, possessing both the largest leverage and studentized residual was observation 40 (SC), with an alarming high studentized residual larger than 3. Nevertheless, one should provide more sociological reasoning if this outlier needed to be removed.    

```{r t8,echo=F}
ols_regress(inv_MA~B+DI+D+INC, data = dt2.d,iterm = T)
```

![Graph 12. Residual diagnostic of the chose model for 2nd problem](g12.tiff)   

![Graph 13. Influential point and outlier diagnostic of the chose model for 2nd problem](g13.png)   

![Graph 14. DFBETAs panel of the chose model for 2nd problem](g14.tiff)    

When doing the inference on the effect sizes predicted by our built model (Table 8), we discovered that the Divorce rate per 10,000 (D) serve as the most impactful independent variable to the inverse of Marriage rate per 10,000 (1/MA), whilst all of the rest effect sizes were close to each other, which seemed natural. On the aspect of the sign of the effects, it made perfect sense that a higher Births per 1000 (B), would stimulate a higher Marriage rate per 10,000 (MA), though it was not significant. It also made some sense that a lower Per capita income expressed in 1972 dollars (INC) would stimulate a higher Marriage rate per 10,000 (MA), since folks richer classes probably have more choices in their life styles rather than getting married. Moreover, we found that a lower Death rate from diabetes, 1978, per 100,000 (DI) would stimulate a higher Marriage rate per 10,000 (MA), maybe our respected client could infer the cause behind this phenomenon. At last, it was a bit shocking to discover that a higher Divorce rate per 10,000 (D) would also stimulate a higher Marriage rate per 10,000 (MA). Perhaps there existed some physiological explanations on a certain group of people tend to rush to all marriage related decisions, regardless of marrying someone of divorcing someone. Either way, this issue should be brought to our respected client's full attention.  

4. Conclusions  

For this project, in order to construct both accurate and interpretable linear regression models on the desired responses, we first conducted appropriate transformations on the response variable and predictors until it could pass the Near-neighbor approach based model adequacy test. Next, we performed step-wise both-direction variable selections with p-values, AIC, adj-R^2^, and C(p) as criteria for a comprehensive consideration. Then, the selected independent variables needed to pass the multicollinearity test. Eventually, linear regression models were constructed and ensued by residual and influence diagnostics. As a result, we discovered that higher D, PL, UR, INC plus lower HS and VT will likely lead to increases in the murder rate (M) in our first model with an adj-R^2^ of 0.843. Meanwhile, the second model revealed that higher D plus lower B, DI and INC will likely lead to increases in the marriage rate (MA) with an adj-R^2^ of 0.675. Moreover, we offered our amateur two-cents towards the interpretation of the models. In the last but not the least, we highly encouraged special attention should be allocated to the outliers and influential points we found by our respected clients.  









```{r g1,  eval=F}
temp = dt[,2:27]
temp = temp[,lapply(.SD,scale)]
temp = melt(temp)

colnames(temp) = c('variable','studentized_value')
ggboxplot(temp, 
          x='variable', y = 'studentized_value',
          color = 'variable', fill = 'Grey',size  = 1, show.legend = F)+ theme(text = element_text(size=26, face = 'bold'))
g2 = list()
for (i in 2:ncol(dt)){
   g2[[i-1]] <- ggplot(dt, aes_string(x=colnames(dt)[i]), x.lab, ) +          geom_histogram(bins = 20,na.rm = T,boundary = .5,color = 'black',fill= i) + theme(text = element_text(size=26, face = 'bold'))
}
ggarrange(g2[[1]],g2[[2]],g2[[3]],g2[[4]],g2[[5]],g2[[6]],g2[[7]],
          g2[[8]],g2[[9]],g2[[10]],g2[[11]],g2[[12]],g2[[13]],g2[[14]],
          g2[[15]],g2[[16]],g2[[17]],g2[[18]],g2[[19]],g2[[20]],
          g2[[21]],g2[[22]],g2[[23]],g2[[24]],g2[[25]],g2[[26]],ncol = 4,nrow = 7)
```
```{r g2, eval=F}
temp1 = dt1[,1:2]
p1 = ggplot(temp1) + geom_point(aes(x=reorder(State, -M), y = M), size = 2, alpha = .8, color = 'blue')+ labs(x = 'State', y = 'M (blue dots)') + theme(text = element_text(size=11, face = 'bold'))
temp2 = dt2[,c(1,2)]
p2 =  ggplot(temp2) + geom_point(aes(x=reorder(State, -MA), y = 1/MA), size = 2, alpha = .8, color = 'red')+ labs(x = 'State', y = '1/MA (red dots)') + theme(text = element_text(size=11, face = 'bold'))
grid.newpage()
grid.draw(rbind(ggplotGrob(p1), ggplotGrob(p2), size = "last"))
```
```{r g3, eval=F}
temp <- ggplot(dt1, aes(x=MA, y=M)) + geom_point() + theme_classic() + geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x11 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=D, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x12 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=PL, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x13 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=S, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x14 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=B, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x15 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=HT, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x16 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=UR, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x17 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=CR, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x18 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=HS, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x19 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=INC, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x110 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1, aes(x=VT, y=M)) + geom_point() + theme_classic()+ geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
x111 = ggMarginal(temp,type = 'histogram', fill = "cyan")
temp <- ggplot(dt1[-33,], aes(x=MA, y=M)) + geom_point() + theme_classic() + geom_smooth(method = 'lm')+ theme(text = element_text(size=18, face = 'bold'))+
        labs(x = 'influential pt temporarily removed MA')
x112 = ggMarginal(temp,type = 'histogram', fill = "cyan")
ggarrange(x11,x112,x12,x13,x14,x15,x16,x17,x18,x19,x110,x111,ncol = 4,nrow = 3)
```
```{r g4,eval = F, warning = F}
plot(k1)
```
```{r g5,eval=F,warning=F}
X.corr.b = cor(dt1.b[,2:7])
corrplot(X.corr.b,method = 'circle',addCoef.col  = 'black')
```
```{r g6, eval=F, warning=F}
g6a=ols_rvsp_plot(model1.b)
g6b=ols_rsd_qqplot(model1.b)
g6c=ols_rsd_hist(model1.b)
g6d=ols_rsd_boxplot(model1.b)
ggarrange(g6a$plot,g6b$plot,g6c$plot,g6d$plot,ncol = 2,nrow = 2)
```
```{r g7, eval=F, warning=F}
g7a=ols_cooksd_chart(model1.b)
g7b=ols_srsd_chart(model1.b)
g7c=ols_dsrvsp_plot(model1.b)
g7d=ols_rsdlev_plot(model1.b)
```
```{r g9, eval=F, warning=F}
g9 = list()
for (i in 3:ncol(dt2.a)){
  temp <- ggplot(dt2.a, aes_string(x=colnames(dt2.a)[i], y = colnames(dt2.a)[2])) + labs(y ='1/MA') +
    geom_point() + theme_classic() + geom_smooth(method = 'lm')+ theme(text = element_text(size=26, face = 'bold'))
  g9[[i-2]] <- ggMarginal(temp,type = 'histogram', fill = "cyan")
}
ggarrange(g9[[1]],g9[[2]],g9[[3]],g9[[4]],g9[[5]],g9[[6]],g9[[7]],
          g9[[8]],g9[[9]],g9[[10]],g9[[11]],g9[[12]],g9[[13]],g9[[14]],
          g9[[15]],g9[[16]],g9[[17]],g9[[18]],g9[[19]],g9[[20]],
          g9[[21]],g9[[22]],g9[[23]],g9[[24]],g9[[25]],ncol = 5,nrow = 5)

```
```{r g10, eval=F, warning=F}
boxcox(model2)
```
```{r g8, eval=F, warning=F}
ols_dfbetas_panel(model1.b)
```
```{r g11, eval=F, warning=F}
X.corr.d = cor(dt2.d[,2:5])
corrplot(X.corr.d,method = 'circle',addCoef.col  = 'black')
```
```{r g12, eval=F, warning=F}
g12a=ols_rvsp_plot(model2.d)
g12b=ols_rsd_qqplot(model2.d)
g12c=ols_rsd_hist(model2.d)
g12d=ols_rsd_boxplot(model2.d)
ggarrange(g12a$plot,g12b$plot,g12c$plot,g12d$plot,ncol = 2,nrow = 2)
```
```{r g13, eval=F, warning=F}
g13a=ols_cooksd_chart(model2.d)
g13b=ols_srsd_chart(model2.d)
g13c=ols_dsrvsp_plot(model2.d)
g13d=ols_rsdlev_plot(model2.d)
```
```{r g14, eval=F, warning=F}
ols_dfbetas_panel(model2.d)
```